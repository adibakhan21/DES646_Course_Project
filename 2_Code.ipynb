{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbsGLeULJH2t",
        "outputId": "d02fd6f4-36fc-487c-f7e5-0c1c8ed3efaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training for 1000 episodes...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gym\n",
        "from gym import spaces\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# --- 1. Custom IITK Campus Environment ---\n",
        "# We simplify the campus into a 10x10 grid\n",
        "# 0: Road, 1: Obstacle (Building/Grass), 2: Start, 3: Goal\n",
        "\n",
        "class IITKCampusEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gym Environment for IIT Kanpur Campus Navigation.\n",
        "    Simplified as a 2D Grid World.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(IITKCampusEnv, self).__init__()\n",
        "\n",
        "        self.grid_size = 10\n",
        "        # Define the grid map\n",
        "        self.grid = np.zeros((self.grid_size, self.grid_size))\n",
        "        # Obstacles\n",
        "        self.grid[1, 1:4] = 1\n",
        "        self.grid[3, 4:8] = 1\n",
        "        self.grid[5, 1:3] = 1\n",
        "        self.grid[7, 7:9] = 1\n",
        "        self.grid[8, 2:5] = 1\n",
        "\n",
        "        # Start (Library) and Goal (LCH)\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (9, 9)\n",
        "        self.grid[self.start_pos] = 2\n",
        "        self.grid[self.goal_pos] = 3\n",
        "\n",
        "        self.agent_pos = self.start_pos\n",
        "\n",
        "        # Action space: 0:Up, 1:Down, 2:Left, 3:Right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        # Observation space: the grid itself\n",
        "        self.observation_space = spaces.Box(low=0, high=3,\n",
        "                                            shape=(self.grid_size, self.grid_size), dtype=np.uint8)\n",
        "\n",
        "        self.max_steps = 100 # Max steps per episode\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.current_step = 0\n",
        "        # Return the observation as a flattened array for the NN\n",
        "        return self.grid.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        x, y = self.agent_pos\n",
        "\n",
        "        if action == 0:  # Up\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 1:  # Down\n",
        "            x = min(self.grid_size - 1, x + 1)\n",
        "        elif action == 2:  # Left\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 3:  # Right\n",
        "            y = min(self.grid_size - 1, y + 1)\n",
        "\n",
        "        new_pos = (x, y)\n",
        "\n",
        "        # Check new position\n",
        "        if new_pos == self.goal_pos:\n",
        "            reward = 1000.0  # Large reward for reaching the goal\n",
        "            done = True\n",
        "        elif self.grid[new_pos] == 1:  # Hit obstacle\n",
        "            reward = -100.0   # Large penalty for crashing (Rsafety)\n",
        "            done = False       # Don't end episode, just penalize\n",
        "            new_pos = self.agent_pos # Don't move\n",
        "        else:\n",
        "            reward = -0.1      # Small penalty for each step (Rtime)\n",
        "            done = False\n",
        "            self.agent_pos = new_pos # Update agent position\n",
        "\n",
        "        # Check for max steps\n",
        "        if self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "            reward = -10.0 # Penalty for timeout\n",
        "\n",
        "        return self.grid.flatten(), reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # Simple console render\n",
        "        render_grid = np.copy(self.grid)\n",
        "        render_grid[self.agent_pos] = 4 # 4 represents the agent\n",
        "        print(render_grid)\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "# --- 2. Deep Q-Network (DQN) Agent ---\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000) # Experience Replay buffer\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.95    # Discount rate\n",
        "        self.epsilon = 1.0   # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model() # Target network\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        # Input layer is flattened grid\n",
        "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear')) # Output Q-values\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # Copy weights from model to target_model\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # Add experience to replay buffer\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size) # Explore\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # Exploit\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        # Train from experience replay\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = self.model.predict(state)\n",
        "\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                # DDQN update: Q(s,a) = r + y * Q_target(s', argmax_a'(Q_main(s',a')))\n",
        "                action_main = np.argmax(self.model.predict(next_state)[0])\n",
        "                target[0][action] = reward + self.gamma * self.target_model.predict(next_state)[0][action_main]\n",
        "\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# --- 3. Training Loop ---\n",
        "if __name__ == \"__main__\":\n",
        "    env = IITKCampusEnv()\n",
        "    state_size = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    EPISODES = 1000 # Reduced for quick demo, set to 50,000 for full training\n",
        "    BATCH_SIZE = 32\n",
        "    UPDATE_TARGET_EVERY = 5 # Episodes\n",
        "\n",
        "    print(f\"Starting Training for {EPISODES} episodes...\")\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        total_reward = 0\n",
        "\n",
        "        for time in range(env.max_steps):\n",
        "            # env.render() # Uncomment to see the grid (slows training)\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Train the agent from memory\n",
        "        if len(agent.memory) > BATCH_SIZE:\n",
        "            agent.replay(BATCH_SIZE)\n",
        "\n",
        "        # Update target network\n",
        "        if e % UPDATE_TARGET_EVERY == 0:\n",
        "            agent.update_target_model()\n",
        "\n",
        "        print(f\"Episode: {e+1}/{EPISODES}, Score: {total_reward}, Epsilon: {agent.epsilon:.2}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    # Add code here to save the model weights\n",
        "    # agent.model.save_weights(\"dqn_iitk_shuttle.h5\")"
      ]
    }
  ]
}